{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49b610d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取CSV文件\n",
    "df = pd.read_csv('/total/data_1.csv')\n",
    "\n",
    "# 去除有缺失值的行\n",
    "df_clean = df.dropna()\n",
    "\n",
    "# 保存为新的CSV文件\n",
    "# df_clean.to_csv('//hy-tmp/data_csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "186b1754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42372, 1035)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a2b8efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分离特征和标签（前1034列是特征，最后一列是标签）\n",
    "features = df.iloc[:, :1034]  # 前1034列\n",
    "label = df.iloc[:, 1034:]     # 最后一列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56f24bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "删除行数: 11184\n",
      "\n",
      "处理后的缺失值统计（前5列示例）:\n",
      "2014/1/1    0\n",
      "2014/1/2    0\n",
      "2014/1/3    0\n",
      "2014/1/4    0\n",
      "2014/1/5    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1. 删除缺失值超过一半的行（基于特征列判断）\n",
    "half_features = 1034 // 2\n",
    "mask = features.isnull().sum(axis=1) <= half_features  # 标记有效行\n",
    "features_cleaned = features[mask].copy()  # 保留有效行\n",
    "label_cleaned = label[mask].copy()        # 对应保留的标签\n",
    "\n",
    "print(f\"删除行数: {len(features) - len(features_cleaned)}\")\n",
    "\n",
    "# 2. 用所在行的均值填充剩余空值\n",
    "row_means = features_cleaned.mean(axis=1, skipna=True)\n",
    "features_filled = features_cleaned.T.fillna(row_means).T  # 先转置再填充，最后转回\n",
    "\n",
    "# 3. 合并处理后的特征和原始标签\n",
    "processed_df = pd.concat([features_filled, label_cleaned], axis=1)\n",
    "\n",
    "# 验证处理结果\n",
    "print(\"\\n处理后的缺失值统计（前5列示例）:\")\n",
    "print(features_filled.iloc[:, :5].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99c110ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理后缺失值总数: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"处理后缺失值总数:\", features_filled.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e46a7b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理前缺失值总数: 32293\n",
      "处理后缺失值总数: 0\n",
      "\n",
      "示例数据（前2行）：\n",
      "    2014/1/1   2014/1/2   2014/1/3   2014/1/4   2014/1/5   2014/1/6  \\\n",
      "0  13.898342  13.898342  13.898342  13.898342  13.898342  13.898342   \n",
      "4   2.900000   5.640000   6.990000   3.320000   3.610000   5.350000   \n",
      "\n",
      "    2014/1/7   2014/1/8   2014/1/9  2014/1/10  ...  2016/10/23  2016/10/24  \\\n",
      "0  13.898342  13.898342  13.898342  13.898342  ...        8.07        8.09   \n",
      "4   4.730000   3.680000   3.530000   3.420000  ...       10.22        8.47   \n",
      "\n",
      "   2016/10/25  2016/10/26  2016/10/27  2016/10/28  2016/10/29  2016/10/30  \\\n",
      "0        9.53        5.48        8.75        9.30        7.54        9.16   \n",
      "4        6.11        6.10        6.73        7.52       10.89        9.86   \n",
      "\n",
      "   2016/10/31  FLAG  \n",
      "0        6.74     1  \n",
      "4        8.72     1  \n",
      "\n",
      "[2 rows x 1035 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取CSV文件\n",
    "\n",
    "df = processed_df\n",
    "# 分离特征和标签（前1034列是特征，最后一列是标签）\n",
    "features = df.iloc[:, :1034]  # 前1034列\n",
    "label = df.iloc[:, 1034:]     # 最后一列\n",
    "\n",
    "# 1. 计算每行均值（仅计算非缺失值）\n",
    "row_means = features.mean(axis=1, skipna=True)\n",
    "\n",
    "# 2. 用行均值填充缺失值\n",
    "features_filled = features.T.fillna(row_means).T  # 转置填充后再转回\n",
    "\n",
    "# 3. 合并处理后的特征和原始标签\n",
    "processed_df = pd.concat([features_filled, label], axis=1)\n",
    "\n",
    "# 验证处理结果\n",
    "print(\"处理前缺失值总数:\", features.isnull().sum().sum())\n",
    "print(\"处理后缺失值总数:\", features_filled.isnull().sum().sum())\n",
    "\n",
    "# 显示处理示例\n",
    "print(\"\\n示例数据（前2行）：\")\n",
    "print(processed_df.head(2))\n",
    "\n",
    "# 可选：保存处理后的数据\n",
    "# processed_df.to_csv('/hy-tmp/data_1_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03912a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df.to_csv('/hy-tmp/data_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8932ba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31188, 1035)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "668022da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to /hy-tmp/dataset_rate_0605_realy/\n",
      "Dataset info written to /hy-tmp/dataset_rate_0605_realy/dataset_info.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "\n",
    "# 配置路径\n",
    "# input_csv = \"/TS-TCC/data_preprocessing/epilepsy/data_files/0329data.csv\"\n",
    "output_dir = \"/hy-tmp/dataset_rate_0605_realy/\"\n",
    "os.makedirs(output_dir, exist_ok=True)  # 确保输出目录存在\n",
    "\n",
    "# 读取数据\n",
    "data = pd.read_csv('/total/data_2.csv')\n",
    "y = data.iloc[:, -1]\n",
    "x = data.iloc[:, :-1]\n",
    "\n",
    "# 数据预处理\n",
    "x = x.to_numpy()\n",
    "y = y.to_numpy()  # 假设原始标签从1开始，调整为0开始\n",
    "# y = (y != 0).astype(int)  # 将所有非0类别合并为1（二分类问题）\n",
    "\n",
    "# 归一化\n",
    "scaler = MinMaxScaler()\n",
    "x = scaler.fit_transform(x)\n",
    "\n",
    "# 数据集划分\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_val = X_test\n",
    "y_val = y_test\n",
    "\n",
    "X_train_2d = X_train.reshape(X_train.shape[0], -1)\n",
    "ada = ADASYN(sampling_strategy='minority', random_state=42)\n",
    "X_train, y_train = ada.fit_resample(X_train_2d, y_train)\n",
    "X_train = X_train.reshape(X_train.shape[0], 1034)\n",
    "X_train = X_train\n",
    "y_train = y_train\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "# 保存为.pt文件\n",
    "def save_torch_data(X, y, filename):\n",
    "    dat_dict = {\n",
    "        \"samples\": torch.from_numpy(X).unsqueeze(1).float(),  # 添加通道维度并转为float\n",
    "        \"labels\": torch.from_numpy(y).long()                  # 标签转为long\n",
    "    }\n",
    "    torch.save(dat_dict, os.path.join(output_dir, filename))\n",
    "\n",
    "save_torch_data(X_train, y_train, \"train.pt\")\n",
    "save_torch_data(X_val, y_val, \"val.pt\")\n",
    "save_torch_data(X_test, y_test, \"test.pt\")\n",
    "\n",
    "# 生成描述文件\n",
    "def get_dataset_info(X, y, name):\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"samples\": len(X),\n",
    "        \"features\": X.shape[1],\n",
    "        \"classes\": dict(Counter(y)),\n",
    "        \"data_shape\": f\"{X.shape[0]} samples × {X.shape[1]} features\"\n",
    "    }\n",
    "\n",
    "info = {\n",
    "    \"dataset\": \"Epilepsy Classification\",\n",
    "    # \"source\": input_csv,\n",
    "    \"preprocessing\": \"MinMaxScaler normalization, binary classification (0 vs non-0)\",\n",
    "    \"train\": get_dataset_info(X_train, y_train, \"train\"),\n",
    "    \"val\": get_dataset_info(X_val, y_val, \"val\"),\n",
    "    \"test\": get_dataset_info(X_test, y_test, \"test\")\n",
    "}\n",
    "\n",
    "# 写入txt文件\n",
    "with open(os.path.join(output_dir, \"dataset_info.txt\"), \"w\") as f:\n",
    "    f.write(\"=== Dataset Information ===\\n\")\n",
    "    # f.write(f\"Source CSV: {info['source']}\\n\")\n",
    "    f.write(f\"Preprocessing: {info['preprocessing']}\\n\\n\")\n",
    "    \n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        data = info[split]\n",
    "        f.write(f\"--- {data['name'].upper()} SET ---\\n\")\n",
    "        f.write(f\"Samples: {data['samples']}\\n\")\n",
    "        f.write(f\"Shape: {data['data_shape']}\\n\")\n",
    "        f.write(f\"Classes: {data['classes']}\\n\\n\")\n",
    "\n",
    "    f.write(f\"Total samples: {len(x)}\\n\")\n",
    "    f.write(f\"Original features: {x.shape[1]}\\n\")\n",
    "    f.write(f\"Final tensor shape: [samples, 1, features] (added channel dim)\\n\")\n",
    "\n",
    "print(f\"Data saved to {output_dir}\")\n",
    "print(f\"Dataset info written to {os.path.join(output_dir, 'dataset_info.txt')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dea2a40b-25d5-4f43-bffc-dd5edc50bc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 读取原始数据\n",
    "df = pd.read_csv(\"/total/data_1.csv\")  # 替换为你的路径\n",
    "\n",
    "# Step 1: 删除缺失值超过30%的样本\n",
    "threshold = int(0.3 * 1034)\n",
    "df_cleaned = df[df.iloc[:, :-1].isnull().sum(axis=1) <= threshold].copy()\n",
    "\n",
    "# Step 2: 对剩余缺失值，按每户家庭的平均值填充\n",
    "df_cleaned.iloc[:, :-1] = df_cleaned.iloc[:, :-1].apply(lambda row: row.fillna(row.mean()), axis=1)\n",
    "\n",
    "# Step 3: 删除用电全为0的样本\n",
    "df_cleaned = df_cleaned[~(df_cleaned.iloc[:, :-1] == 0).all(axis=1)]\n",
    "\n",
    "# Step 4: 对每户用电进行z-score标准化（按行标准化）\n",
    "usage_data = df_cleaned.iloc[:, :-1]\n",
    "usage_normalized = (usage_data - usage_data.mean(axis=1).values[:, None]) / usage_data.std(axis=1).values[:, None]\n",
    "df_cleaned.iloc[:, :-1] = usage_normalized\n",
    "\n",
    "# Step 5: 保存处理后的数据\n",
    "df_cleaned.to_csv(\"/total/cleaned_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f2aab5e-621f-4219-a309-0c53971fe94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to /hy-tmp/0701_dataset/\n",
      "Dataset info written to /hy-tmp/0701_dataset/dataset_info.txt\n"
     ]
    }
   ],
   "source": [
    "# 采取ADASYN方法处理类不平衡的问题\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "# 配置路径\n",
    "# input_csv = \"/TS-TCC/data_preprocessing/epilepsy/data_files/0329data.csv\"\n",
    "output_dir = \"/hy-tmp/0701_dataset/\"\n",
    "os.makedirs(output_dir, exist_ok=True)  # 确保输出目录存在\n",
    "\n",
    "# 读取数据\n",
    "data = pd.read_csv('/total/data_1_knnimpute.csv')\n",
    "y = data.iloc[:, -1]\n",
    "x = data.iloc[:, :-1]\n",
    "\n",
    "# 数据预处理\n",
    "x = x.to_numpy()\n",
    "y = y.to_numpy()  # 假设原始标签从1开始，调整为0开始\n",
    "# y = (y != 0).astype(int)  # 将所有非0类别合并为1（二分类问题）\n",
    "\n",
    "# 归一化\n",
    "scaler = MinMaxScaler()\n",
    "x = scaler.fit_transform(x)\n",
    "\n",
    "# 数据集划分\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_val = X_test\n",
    "y_val = y_test\n",
    "\n",
    "X_train_2d = X_train.reshape(X_train.shape[0], -1)\n",
    "\n",
    "smote_enn = SMOTEENN(sampling_strategy='auto', random_state=42)\n",
    "ada = ADASYN(sampling_strategy='minority', random_state=42)\n",
    "X_train, y_train = smote_enn.fit_resample(X_train_2d, y_train)\n",
    "X_train = X_train.reshape(X_train.shape[0], 1034)\n",
    "\n",
    "\n",
    "# 保存为.pt文件\n",
    "def save_torch_data(X, y, filename):\n",
    "    dat_dict = {\n",
    "        \"samples\": torch.from_numpy(X).unsqueeze(1).float(),  # 添加通道维度并转为float\n",
    "        \"labels\": torch.from_numpy(y).long()                  # 标签转为long\n",
    "    }\n",
    "    torch.save(dat_dict, os.path.join(output_dir, filename))\n",
    "\n",
    "save_torch_data(X_train, y_train, \"train.pt\")\n",
    "save_torch_data(X_val, y_val, \"val.pt\")\n",
    "save_torch_data(X_test, y_test, \"test.pt\")\n",
    "\n",
    "# 生成描述文件\n",
    "def get_dataset_info(X, y, name):\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"samples\": len(X),\n",
    "        \"features\": X.shape[1],\n",
    "        \"classes\": dict(Counter(y)),\n",
    "        \"data_shape\": f\"{X.shape[0]} samples × {X.shape[1]} features\"\n",
    "    }\n",
    "\n",
    "info = {\n",
    "    \"dataset\": \"Epilepsy Classification\",\n",
    "    # \"source\": input_csv,\n",
    "    \"preprocessing\": \"MinMaxScaler normalization, binary classification (0 vs non-0)\",\n",
    "    \"train\": get_dataset_info(X_train, y_train, \"train\"),\n",
    "    \"val\": get_dataset_info(X_val, y_val, \"val\"),\n",
    "    \"test\": get_dataset_info(X_test, y_test, \"test\")\n",
    "}\n",
    "\n",
    "# 写入txt文件\n",
    "with open(os.path.join(output_dir, \"dataset_info.txt\"), \"w\") as f:\n",
    "    f.write(\"=== Dataset Information ===\\n\")\n",
    "    f.write(f\"描述: 读取数据_归一化_划分数据集_smote类不平衡处理\\n\")\n",
    "    # f.write(f\"Source CSV: {info['source']}\\n\")\n",
    "    f.write(f\"Preprocessing: {info['preprocessing']}\\n\\n\")\n",
    "    \n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        data = info[split]\n",
    "        f.write(f\"--- {data['name'].upper()} SET ---\\n\")\n",
    "        f.write(f\"Samples: {data['samples']}\\n\")\n",
    "        f.write(f\"Shape: {data['data_shape']}\\n\")\n",
    "        f.write(f\"Classes: {data['classes']}\\n\\n\")\n",
    "\n",
    "    f.write(f\"Total samples: {len(x)}\\n\")\n",
    "    f.write(f\"Original features: {x.shape[1]}\\n\")\n",
    "    f.write(f\"Final tensor shape: [samples, 1, features] (added channel dim)\\n\")\n",
    "\n",
    "print(f\"Data saved to {output_dir}\")\n",
    "print(f\"Dataset info written to {os.path.join(output_dir, 'dataset_info.txt')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6acdc06-6162-4dfe-ae75-7ba9a43efa3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[缺失值过滤后剩余多少条数据] shape: (31036, 1034)\n",
      "[KNN 插补完成]\n",
      "[保存完成] 路径: /total/data_1_knnimpute.csv\n"
     ]
    }
   ],
   "source": [
    "# 采取ADASYN方法处理类不平衡的问题\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import KNNImputer\n",
    "# from imblearn.combine import SMOTETomek\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "# 配置路径\n",
    "# input_csv = \"/TS-TCC/data_preprocessing/epilepsy/data_files/0329data.csv\"\n",
    "# output_dir = \"/hy-tmp/0626_dataset_2/\"\n",
    "# os.makedirs(output_dir, exist_ok=True)  # 确保输出目录存在\n",
    "\n",
    "# 读取数据\n",
    "data = pd.read_csv('/total/data_1.csv')\n",
    "y = data.iloc[:, -1].to_numpy()  # 假设原始标签从1开始，调整为0开始\n",
    "X = data.iloc[:, :-1]\n",
    "\n",
    "# 数据预处理\n",
    "# ==== 删除缺失值过多的样本 ====\n",
    "X[\"missing_count\"] = X.isnull().sum(axis=1)\n",
    "mask = X[\"missing_count\"] <= 500\n",
    "X = X[mask].drop(columns=[\"missing_count\"])\n",
    "y = y[mask.to_numpy()]\n",
    "print(f\"[缺失值过滤后剩余多少条数据] shape: {X.shape}\")\n",
    "\n",
    "# ==== 可选使用 KNN 插补缺失值 ====\n",
    "\n",
    "X = KNNImputer(n_neighbors=5).fit_transform(X)\n",
    "print(\"[KNN 插补完成]\")\n",
    "\n",
    "# ==== 保存处理后的数据 ====\n",
    "output_df = pd.DataFrame(X)\n",
    "output_df['label'] = y\n",
    "output_path = '/total/data_1_knnimpute.csv'\n",
    "output_df.to_csv(output_path, index=False)\n",
    "print(f\"[保存完成] 路径: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f9d0ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to /hy-tmp/0712_realdata/\n",
      "Dataset info written to /hy-tmp/0712_realdata/dataset_info.txt\n"
     ]
    }
   ],
   "source": [
    "# # 归一化\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import KNNImputer\n",
    "# from imblearn.combine import SMOTETomek\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tsfresh import extract_features, select_features\n",
    "import pandas as pd\n",
    "# 读取数据\n",
    "data = pd.read_csv('/hy-tmp/data0702/data_1_knnimpute.csv')\n",
    "y = data.iloc[:, -1].to_numpy()  # 假设原始标签从1开始，调整为0开始\n",
    "X = data.iloc[:, :-1]\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "name = '数据描述:0712读取knn填充后的数据集,不做类不平衡处理'\n",
    "# 数据集划分\n",
    "output_dir = \"/hy-tmp/0712_realdata/\"\n",
    "os.makedirs(output_dir, exist_ok=True)  # 确保输出目录存在\n",
    "\n",
    "\n",
    "# ada = SMOTEENN(sampling_strategy='auto', random_state=42)\n",
    "# X_2d = X.reshape(X.shape[0], -1)\n",
    "# X, y = ada.fit_resample(X_2d, y)\n",
    "# X = X.reshape(X.shape[0], 1034)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_val ,y_val = X_test, y_test\n",
    "\n",
    "# X_train_2d = X_train.reshape(X_train.shape[0], -1)\n",
    "\n",
    "# # smote_enn = SMOTEENN(sampling_strategy='auto', random_state=42)\n",
    "# ada = ADASYN(sampling_strategy='minority', random_state=42)\n",
    "# X_train, y_train = ada.fit_resample(X_train_2d, y_train)\n",
    "# X_train = X_train.reshape(X_train.shape[0], 1034)\n",
    "# X_train = X_train\n",
    "# y_train = y_train\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "# print(\"[FRESH] 正在提取时序特征...\")\n",
    "# df_long = pd.DataFrame(X_train)\n",
    "# df_long['id'] = df_long.index\n",
    "# df_long = df_long.melt(id_vars='id', var_name='time', value_name='value')\n",
    "\n",
    "# features_extracted = extract_features(df_long, column_id='id', column_sort='time', n_jobs=3)\n",
    "# impute(features_extracted, show_warnings=False)\n",
    "# selected_features = select_features(features_extracted, y_train)\n",
    "\n",
    "# X_train_final = selected_features.to_numpy()\n",
    "# print(f\"[FRESH完成] 最终特征维度: {X_train_final.shape[1]}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 保存为.pt文件\n",
    "def save_torch_data(X, y, filename):\n",
    "    dat_dict = {\n",
    "        \"samples\": torch.from_numpy(X).unsqueeze(1).float(),  # 添加通道维度并转为float\n",
    "        \"labels\": torch.from_numpy(y).long()                  # 标签转为long\n",
    "    }\n",
    "    torch.save(dat_dict, os.path.join(output_dir, filename))\n",
    "\n",
    "save_torch_data(X_train, y_train, \"train.pt\")\n",
    "save_torch_data(X_val, y_val, \"val.pt\")\n",
    "save_torch_data(X_test, y_test, \"test.pt\")\n",
    "\n",
    "# 生成描述文件\n",
    "def get_dataset_info(X, y, name):\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"samples\": len(X),\n",
    "        \"features\": X.shape[1],\n",
    "        \"classes\": dict(Counter(y)),\n",
    "        \"data_shape\": f\"{X.shape[0]} samples X {X.shape[1]} features\"\n",
    "    }\n",
    "\n",
    "info = {\n",
    "    \"dataset\": \"Epilepsy Classification\",\n",
    "    # \"source\": input_csv,\n",
    "    \"preprocessing\": \"MinMaxScaler normalization, binary classification (0 vs non-0)\",\n",
    "    \"train\": get_dataset_info(X_train, y_train, \"train\"),\n",
    "    \"val\": get_dataset_info(X_val, y_val, \"val\"),\n",
    "    \"test\": get_dataset_info(X_test, y_test, \"test\")\n",
    "}\n",
    "\n",
    "\n",
    "# 写入txt文件\n",
    "with open(os.path.join(output_dir, \"dataset_info.txt\"), \"w\") as f:\n",
    "    f.write(\"=== Dataset Information ===\\n\")\n",
    "    f.write(f\"{name}\\n\")\n",
    "    # f.write(f\"Source CSV: {info['source']}\\n\")\n",
    "    f.write(f\"Preprocessing: {info['preprocessing']}\\n\\n\")\n",
    "    \n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        data = info[split]\n",
    "        f.write(f\"--- {data['name'].upper()} SET ---\\n\")\n",
    "        f.write(f\"Samples: {data['samples']}\\n\")\n",
    "        f.write(f\"Shape: {data['data_shape']}\\n\")\n",
    "        f.write(f\"Classes: {data['classes']}\\n\\n\")\n",
    "\n",
    "    f.write(f\"Total samples: {len(X)}\\n\")\n",
    "    f.write(f\"Original features: {X.shape[1]}\\n\")\n",
    "    f.write(f\"Final tensor shape: [samples, 1, features] (added channel dim)\\n\")\n",
    "\n",
    "print(f\"Data saved to {output_dir}\")\n",
    "print(f\"Dataset info written to {os.path.join(output_dir, 'dataset_info.txt')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1503f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "save_torch_data(X_train_final, y_train, \"train.pt\")\n",
    "save_torch_data(X_val, y_val, \"val.pt\")\n",
    "save_torch_data(X_test, y_test, \"test.pt\")\n",
    "\n",
    "\n",
    "info = {\n",
    "    \"dataset\": \"Epilepsy Classification\",\n",
    "    # \"source\": input_csv,\n",
    "    \"preprocessing\": \"MinMaxScaler normalization, binary classification (0 vs non-0)\",\n",
    "    \"train\": get_dataset_info(X_train_final, y_train, \"train\"),\n",
    "    \"val\": get_dataset_info(X_val, y_val, \"val\"),\n",
    "    \"test\": get_dataset_info(X_test, y_test, \"test\")\n",
    "}\n",
    "output_dir = \"/hy-tmp/0629dataset_featurn/\"\n",
    "\n",
    "# 写入txt文件\n",
    "with open(os.path.join(output_dir, \"dataset_info.txt\"), \"w\") as f:\n",
    "    f.write(\"=== Dataset Information ===\\n\")\n",
    "    # f.write(f\"Source CSV: {info['source']}\\n\")\n",
    "    f.write(f\"Preprocessing: {info['preprocessing']}\\n\\n\")\n",
    "    \n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        data = info[split]\n",
    "        f.write(f\"--- {data['name'].upper()} SET ---\\n\")\n",
    "        f.write(f\"Samples: {data['samples']}\\n\")\n",
    "        f.write(f\"Shape: {data['data_shape']}\\n\")\n",
    "        f.write(f\"Classes: {data['classes']}\\n\\n\")\n",
    "\n",
    "    f.write(f\"Total samples: {len(X)}\\n\")\n",
    "    f.write(f\"Original features: {X.shape[1]}\\n\")\n",
    "    f.write(f\"Final tensor shape: [samples, 1, features] (added channel dim)\\n\")\n",
    "\n",
    "print(f\"Data saved to {output_dir}\")\n",
    "print(f\"Dataset info written to {os.path.join(output_dir, 'dataset_info.txt')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
